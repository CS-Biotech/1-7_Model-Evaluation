{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Model Evaluation üîç\n",
    "# Tutorial\n",
    "\n",
    "In the previous week, we trained a model using the training data and improved the model by tuning hyperparameters. This week, we will use the test data to do a final evaluation of how well our model performs on unseen data.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "We will discuss the following topics and how to use the evaluation metrics in sklearn:\n",
    "1. Generalization\n",
    "    * Case Study\n",
    "3. Underfitting/overfitting\n",
    "4. Evaluation metrics (covered in this week's pre-module exercise)\n",
    "    * Accuracy\n",
    "    * Confusion Matrix\n",
    "    * Precision & Recall\n",
    "    * F1-score\n",
    "\n",
    "We will then evaluate our trained breast cancer model using our test set and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, run the cell below to load a trained model and the dataset (split into train and test). This model has been trained similarly to your work in the week 6 tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_csv('bc_data.csv', index_col=0)\n",
    "\n",
    "# Data cleaning\n",
    "# remove the 'Unnamed: 32' column\n",
    "df = df.drop('Unnamed: 32', axis=1)\n",
    "\n",
    "# encode target feature to binary class and split target/predictor vars\n",
    "y = df[\"diagnosis\"].map({\"B\" : 0, \"M\" : 1})\n",
    "x = df.drop(\"diagnosis\", axis = 1)\n",
    "\n",
    "# drop all \"worst\" columns\n",
    "cols = ['radius_worst', \n",
    "        'texture_worst', \n",
    "        'perimeter_worst', \n",
    "        'area_worst', \n",
    "        'smoothness_worst', \n",
    "        'compactness_worst', \n",
    "        'concavity_worst',\n",
    "        'concave points_worst', \n",
    "        'symmetry_worst', \n",
    "        'fractal_dimension_worst']\n",
    "x = x.drop(cols, axis=1)\n",
    "\n",
    "# drop perimeter and area (keep radius)\n",
    "cols = ['perimeter_mean',\n",
    "        'perimeter_se', \n",
    "        'area_mean', \n",
    "        'area_se']\n",
    "x = x.drop(cols, axis=1)\n",
    "\n",
    "# Data splitting\n",
    "# train (70%), val (15%), test (15%)\n",
    "# val set not needed in this module, but kept for consistency\n",
    "# random_state seed is the same as week 6; resulting sets should be the same\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=40)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_ratio/(train_ratio + val_ratio), random_state=40)\n",
    "\n",
    "# Step 1: Define the true underlying function (hidden from students)\n",
    "def true_function(x):\n",
    "    return 0.02 * (x-2) * x * (x-1) * (x+3) * (x-5) \n",
    "\n",
    "# Load a trained model\n",
    "with open('bc_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "\n",
    "When we talk about model performance, we want to know how well a model <span style=\"background-color: #AFEEEE\">**generalizes**</span>. This measures a model's ability to perform well on **new, unseen data**. Remember that at the beginning of our pipeline, we created hold-out data that we never showed the model during training. We will use this test data in this evaluation stage.\n",
    "\n",
    "To explore this concept, we will explore the concept of polynomial regression, similar to linear or logistic regression explored in Module 5. The idea of linear and polynomial regression is that we want to create a function that best *fits* the data we're looking at. With linear regression, we're finding the line of best fit. With polynomial regression, we can pick a degree for the polynomial and try to fit a polynomial of that degree to the data.\n",
    "\n",
    "As a reminder, a polynomial is defined as $$y = a_0x^0 + a_1x^1 + a_2x^2 + \\dots + a_n x^n$$ with $n$ denoting the degree of the polynomial. So, for example, $y = x^4 + x$ is a 4th degree polynomial, $y = x$ is a first degree polynomial, and $y = 6$ is a zeroth degree polynomial, or a **constant** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Case Study**\n",
    "\n",
    "A novel species of bacteria called *Clostridium twoohonesus* has been found to be responsible for causing a condition called **HMBosis**. This condition is characterized by a high enjoyment of the course content being learned in HMB201. The infected are more prone to taking HMB301 and HMB491 in the future.\n",
    "\n",
    "The bacteria have novel biochemical interactions that are understood to be temperature-dependent, but not fully elucidated. Additionally, a subset of these interactions is known to heavily impact the growth rate of the bacteria.\n",
    "\n",
    "You are a scientist tasked with modelling the growth rate of the bacteria as a function of temperature. You have incubated a culture of the bacteria at different temperatures, but didn't have access to [Pytri AI's](https://www.pytri.ai/) (an HMB491 partner!) automatic colony counter, and had to manually count the number of bacteria in each plate.\n",
    "\n",
    "You have the following data to train your model:\n",
    "\n",
    "| Temperature (C) | Colony Growth Rate |\n",
    "|-----------------|--------------|\n",
    "| - 2.888         |       0.57779262  |\n",
    "| - 1.777         |        2.41393475  | \n",
    "| - 0.666         |        1.39521538   |\n",
    "| 0.444           |       0.9104623    |\n",
    "| 1.555           |       1.0518173    |\n",
    "| 2.666           |        -1.62275662  |\n",
    "| 3.777           |        -3.40006912  |\n",
    "| 4.888           |        -0.63161147  |\n",
    "\n",
    "And you also have the following data to test your model:\n",
    "| Temperature (C) | Colony Growth Rate |\n",
    "|-----------------|--------------|\n",
    "| - 1.5           |          2.87371602  |\n",
    "| 1               |          -1.10633497  |\n",
    "| 3.5             |          -3.25558162  |\n",
    "\n",
    "First, you will need to insert the data into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_train = [-2.888, -1.777, -0.666, 0.444, 1.555, 2.666, 3.777, 4.888]\n",
    "growth_rate_train = [0.57779262, 2.41393475, 1.39521538, 0.9104623, 1.0518173, -1.62275662, -3.40006912, -0.63161147]\n",
    "\n",
    "temperature_test = [-1.5, 1, 3.5]\n",
    "growth_rate_test = [2.87371602, -1.10633497, -3.25558162]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the training data and the test data to get a sense of the data.\n",
    "\n",
    "Make sure to use a grid and label the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(temperature_train, growth_rate_train, label='Training data', color='blue')\n",
    "plt.scatter(temperature_test, growth_rate_test, label='Test data', color='green')\n",
    "plt.xlabel('Temperature (ÀöC)')\n",
    "plt.ylabel('Colony Growth Rate')\n",
    "plt.title('Colony Growth Rate vs Temperature')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for you, a kind colleague at your lab has provided you with the code to perform polynomial regression.\n",
    "\n",
    "The function also reports the average loss of the model. Here, the loss is defined as the absolute difference between the predicted growth rate and the true growth rate at each temperature. This is a pretty good way to determine if our model is accurately predicting the growth rate at each temperature.\n",
    "\n",
    "Your job is now to find the best-fit polynomial for the data. We want to **absolutely minimize the loss**, to get as close to 0 as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(x, y, degree):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    x_poly = poly.fit_transform(np.array(x).reshape(-1, 1))\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(x_poly, y)\n",
    "    \n",
    "    x_plot = np.linspace(-4, 6, 1000).reshape(-1, 1)\n",
    "    x_plot_poly = poly.transform(x_plot)\n",
    "    y_pred = model.predict(x_plot_poly)\n",
    "    \n",
    "    y_train_pred = model.predict(x_poly)\n",
    "    \n",
    "    losses = np.abs(y - y_train_pred)\n",
    "    average_loss = np.mean(losses)\n",
    "    print(f\"Average Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x, y, label='Training data', color='blue')\n",
    "    plt.plot(x_plot, y_pred, label=f'Polynomial fit (degree {degree})', color='red')\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.plot([x[i], x[i]], [y[i], y_train_pred[i]], 'k--', linewidth=0.8)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'Colony Growth Rate vs Temperature (Polynomial Regression (degree {degree}))')\n",
    "    plt.xlabel('Temperature (ÀöC)')\n",
    "    plt.ylabel('Colony Growth Rate')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(-5, 6)  # Restrict y-axis range\n",
    "    plt.show()\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a few different degrees and pick which one **you** think is the best fit. Specifically, pick some low values and some high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 10\n",
    "polynomial_regression(temperature_train, growth_rate_train, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept Check:** What happened when you used a low degree polynomial? What about a high degree polynomial? How did the curve change? How did the loss change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "* Low degree: The curve did not fit the data well. At 1, specifically, we had a downward sloping line, which looked like it could fit the data well.\n",
    "* High degree: The curve fits the training data very well, with a lot of ups and downs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a low-degree polynomial, we saw that the curve did not fit the data well, and had a higher loss. In this case, our model was '**underfitting**' the data. This is when the model is 'too simple' to fit the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should also see how well our model performs on the test data. This will tell us how well our model generalizes to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_test(x_train, y_train, x_test, y_test, degree, true_f=False):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    x_train_poly = poly.fit_transform(np.array(x_train).reshape(-1, 1))\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train_poly, y_train)\n",
    "    \n",
    "    x_test_poly = poly.transform(np.array(x_test).reshape(-1, 1))\n",
    "    \n",
    "    y_train_pred = model.predict(x_train_poly)\n",
    "    y_test_pred = model.predict(x_test_poly)\n",
    "    \n",
    "    train_losses = np.abs(y_train - y_train_pred)\n",
    "    test_losses = np.abs(y_test - y_test_pred)\n",
    "    average_train_loss = np.mean(train_losses)\n",
    "    average_test_loss = np.mean(test_losses)\n",
    "    print(f\"Average Training Loss: {average_train_loss:.4f}\")\n",
    "    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n",
    "\n",
    "    x_plot = np.linspace(-4, 6, 1000).reshape(-1, 1)\n",
    "    x_plot_poly = poly.transform(x_plot)\n",
    "    y_pred = model.predict(x_plot_poly)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_train, y_train, label='Training data', color='blue')\n",
    "    plt.scatter(x_test, y_test, label='Test data', color='green')\n",
    "\n",
    "    if true_f:\n",
    "        plt.plot(x_plot, true_function(x_plot), label=f'True function', color='black')\n",
    "\n",
    "    plt.plot(x_plot, y_pred, label=f'Polynomial fit (degree {degree})', color='red')\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "        plt.plot([x_train[i], x_train[i]], [y_train[i], y_train_pred[i]], 'b--', linewidth=0.8)\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        plt.plot([x_test[i], x_test[i]], [y_test[i], y_test_pred[i]], 'g--', linewidth=0.8)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'Colony Growth Rate vs Temperature (Polynomial Regression (degree {degree}))')\n",
    "    plt.xlabel('Temperature (ÀöC)')\n",
    "    plt.ylabel('Colony Growth Rate')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(-5, 6)  # Restrict y-axis range\n",
    "    plt.show()\n",
    "    \n",
    "    return average_train_loss, average_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression_test(temperature_train, growth_rate_train, temperature_test, growth_rate_test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept Check:** What happened when you used a low-degree polynomial? What about a high-degree polynomial? How did the curve change? How did the training and test loss change?\n",
    "\n",
    "**Answer:** With a low-degree polynomial, the curve did not fit the data well, and the training and test losses were high. With a high degree polynomial, the curve fit the training data very well, but the test loss was high.\n",
    "\n",
    "**Concept Check:** What degree polynomial is the best for modelling this data? Why?\n",
    "\n",
    "**Answer:** I chose a degree 5 polynomial because the curve fit the training data well, and the test loss was lowest among all degrees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now have seen that using a very high-degree polynomial can perfectly fit the training data, but it may not generalize well to unseen test data. This is where the concept of **overfitting** comes in.\n",
    "\n",
    "Overfitting is when a model is too complex and fits the training data too well, so that it captures noise in the training data. This can lead to poor generalization to unseen data.\n",
    "\n",
    "We can see this in the graph above, where the curve fits the training data very well, but the test loss was high. This is because the model is too complex and has 'memorized' the training data, rather than learning the underlying pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Case Study Conclusion**\n",
    "\n",
    "Finally, after investigating the model's performance, you've come to the conclusion that a degree 5 polynomial is the best fit for this data. You can now use this model to make predictions on new data.\n",
    "\n",
    "Through investigations using Michaelis-Menten Kinetics, the growth rate-temperature relationship is found to be:\n",
    "\n",
    "$$ y = 0.02 (x+3)x(x-2)(x-3)(x-5) $$\n",
    "\n",
    "which is indeed a 5th-degree polynomial. Here is the function plotted with the training data and test data, and the predictions of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression_test(temperature_train, growth_rate_train, temperature_test, growth_rate_test, 5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization Summary\n",
    "\n",
    "**Noise**: irrelevant data in the dataset that is not part of the useful pattern that needs to be learned. Outliers, errors, or inconsistencies in the dataset are examples of noise. We say that a dataset is **noisy** if it includes many such datapoints. \n",
    "\n",
    "|  | Definition | How to detect |\n",
    "| --- | --- | --- |\n",
    "| **Underfitting** | a model is not able to capture the useful/important patterns in the dataset. | Poor performance on the training set | \n",
    "| **Overfitting** | a model performs poorly on unseen test data, even though it performed well during training. | Great performance on the training set and poor performance on the test set |\n",
    "\n",
    "When the model performs well on the training set and the test set, we say that it is a **good fit** and **generalizes** well. This means the model has learned from the training set well enough to apply in the general case, to new data it has not seen before.\n",
    "\n",
    "**Generalization**: a model's ability to adapt to and perform well on new, unseen data.\n",
    "A model generalizes well if it performs well on unseen data (i.e., a test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating model predictions\n",
    "\n",
    "**We will revisit the heart failure dataset from Module 6.** First, we need to get the trained model's predictions. Our evaluation metrics will need both the classification (0/1) labels and prediction probabilities. We can do this using the **predict()** function, which can be accessed and called on sklearn model objects. The model that you trained in week 6 is an object of the SGDClassifier class, so this function is available on your model.  \n",
    "\n",
    "| Function | Input Parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| predict() | array of input data points | predicted classifications (0's and 1's) corresponding to each input data point. For example, [0, 1, 1, 0, 0, 0, 1, 0, ... , 1] | model.predict(input_testset) |\n",
    "\n",
    "Documentation for SGDClassifier:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "Scroll down to \"methods\" to see the functions that can be called on objects of this class.\n",
    "\n",
    "**Q1. Complete the code below to call predict() on the model, using your test set inputs.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your test set is saved in variables x_test and y_test.\n",
    "\n",
    "# Prediction\n",
    "preds = ...    #TODO: complete this line\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using evaluation metrics\n",
    "\n",
    "In the pre-module, we talked about 4 metrics we can calculate: accuracy, precision, recall, and the F1-score (along with the confusion matrix). \n",
    "\n",
    "To get these results using sklearn, we need to import some functions from sklearn.metrics.\n",
    "\n",
    "**Run the code below to import the libraries for model metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Confusion Matrix\n",
    "\n",
    "Let's generate the confusion matrix first, using the ```confusion_matrix()``` function provided in scikit-learn.\n",
    "\n",
    "In this table, y_true = array of input data points and y_pred = array of predicted labels.\n",
    "\n",
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| confusion_matrix() | y_true, y_pred | a confusion matrix represented as a 2D array | confusion_matrix(y_test, preds) |\n",
    "\n",
    "**Q2. Complete the code below to generate the confusion matrix.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = ...    #TODO: complete this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Display\n",
    "Right now, if we print the confusion matrix, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nConfusion matrix:\\n\",conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better, clearer visualization with proper axis labels, we can use the ConfusionMatrixDisplay object from sklearn. \n",
    "\n",
    "First we have to import a function from sklearn.metrics, like we did for all the other metrics. The ConfusionMatrixDisplay() function creates an object of the ConfusionMatrixDisplay class, and this object can display/plot the confusion matrix.\n",
    "\n",
    "A little explanation on the input parameters to this function:\n",
    "* confusion_matrix: a confusion matrix object (from sklearn ```confusion_matrix```)\n",
    "* display_labels: an array of axis labels. You can obtain them from your model with ```model.classes_```\n",
    "\n",
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| ConfusionMatrixDisplay() | confusion_matrix, display_labels | a display object for confusion matrix. | ConfusionMatrixDisplay(confusion_matrix, display_labels) |\n",
    "\n",
    "Finally, calling **plot()** on your newly created confusion matrix display object will plot the actual display. \n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "\n",
    "**Q3. Complete the code below to create a confusion matrix display.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "labels = np.where(model.classes_, 'malignant', 'benign')\n",
    "\n",
    "display = ...    # TODO: complete this line\n",
    "\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Identify the number of FP, FN, TP, and TN from the confusion matrix of the breast cancer detection model. Malignant is positive.** \n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "* FP: \n",
    "* FN: \n",
    "* TP: \n",
    "* TN: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. State the ratio of actual positives to actual negatives. Is this dataset balanced?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. Using what you learned in the pre-module, calculate the accuracy, precision, recall, and f1-score for the breast cancer model based on the confusion matrix.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = \n",
    "\n",
    "Precision = \n",
    "\n",
    "Recall = \n",
    "\n",
    "F1-Score = \n",
    "\n",
    "NOTE: Students can look at their completed pre-module work/the formulas presented in that notebook as a guide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Accuracy, Precision, Recall, F1-Score\n",
    "\n",
    "Here is a summary of the sklearn functions for the metrics we covered in the pre-module, as well as their input parameters, output, and syntax. Note that there are additional optional input parameters than listed here, but for this particular binary classification problem, we do not need them. We have only listed the required parameters for each function. \n",
    "\n",
    "In this table, y_true = array of input data points and y_pred = array of predicted labels.\n",
    "\n",
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| accuracy_score() | y_true, y_pred | accuracy (decimal) | accuracy_score(y_test, preds) |\n",
    "| precision_score() | y_true, y_pred | precision (decimal) | precision_score(y_test, preds) | \n",
    "| recall_score() | y_true, y_pred | recall (decimal) | recall_score(y_test, preds) | \n",
    "| f1_score() | y_true, y_pred | f1-score (decimal) | f1_score(y_test, preds) | \n",
    "\n",
    "**Q7. Complete the code below to get each metric and save them to the given variables.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "acc =         #TODO: complete this line\n",
    "precision =   #TODO: complete this line\n",
    "recall =      #TODO: complete this line\n",
    "f1 =          #TODO: complete this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print all our results. **Run the code below**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = pd.Series({\n",
    "    \"Accuracy\": acc,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1-Score\": f1\n",
    "})\n",
    "\n",
    "print(f\"Model: LogisticRegression\")\n",
    "print(f\"\\nEvaluation metrics:\\n\",eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the generated evaluation results for the 4 metrics above. Compare them with your hand-calculated results in question 3- they should be the same. You can use the scikit-learn-generated values to check your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. There is no universal threshold for what constitutes a \"good model\". However, your colleagues at the Breast Cancer Institute have unanimously decided that a model with an F1-score of 0.8 and above will suffice. They believe that using the F1-score to evaluate the model is the best approach, since it combines precision and recall, and the dataset is imbalanced. They are not considering using any other metrics at the moment.** \n",
    "\n",
    "**You disagree with your colleagues on this decision. How would you evaluate the performance differently, and why? What information might you need (from physicians, medical researchers, etc.) to make a recommendation?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Graded Exercise: (5 marks)**\n",
    "\n",
    "**GQ1. (2 marks) Suppose that upon evaluating your model, you find that the accuracy is very low (around 60%). However, the accuracy during training was very good (around 95%). What do you call this phenomenon, and how would you try to correct it? List at least two methods discussed in this module.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ2. (2 marks) Suppose that your model has an accuracy of 62% during training, using a logistic regression model and 200 training samples. After obtaining more samples for your dataset, you retrain the model with 400 training samples, but the accuracy only goes up to 66%. What do you call this phenomenon, and how would you try to correct it? List at least two methods discussed in this module.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ3. (1 mark) Say that one of your colleagues has trained a different model. Their evaluation results look like this:**\n",
    "\n",
    "Colleague's evaluation results:\n",
    "* Accuracy     0.868421\n",
    "* Precision    0.900000\n",
    "* Recall       0.692308\n",
    "* F1-Score     0.782609\n",
    "\n",
    "And your evaluation results look like this:\n",
    "* Accuracy     0.837209\n",
    "* Precision    0.695652\n",
    "* Recall       1.000000\n",
    "* F1-Score     0.820513\n",
    "\n",
    "**Assume that the dataset is imbalanced (more benign cells). Your colleague says that your model performs better than theirs. Describe two reasons, seen in the two evaluation results, to support this claim.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Let's go back to our motivation-- we set out to find out how well a machine learning model could be used to predict which cancer cells are benign or malignant, as a form of early diagnosis to improve patient care. What did we learn? Based on our evaluation, the logistic regression model we trained on this dataset shows potential to be useful as a tool to aid physicians in their diagnosis; an F1-score of 0.82 is generally considered good. However, more information from stakeholders might be needed to quantify exactly how \"good\" the model has to be for it to be feasible in a medical setting. Realizing that these decisions might require expert opinions and interdisciplinary collaboration, you call for a meeting with machine learning researchers, biologists, physicians, and your colleagues at the Canadian Cancer Society. You organize your results and learnings into a report, and head off to the meeting with valuable information that could leave a lasting impact on healthcare.\n",
    "\n",
    "\n",
    "### Recap of ML Pipeline\n",
    "\n",
    "<img src=\"image-8.png\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "<img src=\"image-9.png\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "<img src=\"image-11.png\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "<img src=\"image-12.png\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "\n",
    "Congratulations! You are now able to identify what happens in each stage of a machine learning pipeline and how each stage works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
